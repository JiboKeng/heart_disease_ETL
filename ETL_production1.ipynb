{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bec7366-e1f4-4565-ab9f-d952dcb27402",
   "metadata": {},
   "source": [
    " <div style=\"background-color: #00008B; padding: 10px; color: white;\">\n",
    "<h2 style=\"color: #00BFFF; font-weight: bold;\">optimising the minimum viable product in the heart_disease_ETL_exploratory notebook</h2>\n",
    "<h3>\n",
    "\n",
    "- wrap the code into functions as we want to keep it organised into tasks and it will be repetitive to read the data from the four data sources to form a larger dataset for modelling(the more the data the better) thus a function is appropriate\n",
    "- add source identifier before merging for tracability and docstrings for function documentation\n",
    "- clean master data set and save to csv and then continue with the machine learning life cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ef719784-328b-4a9a-84a3-b9f39e4864a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import kagglehub\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fc22b10a-4bbc-4920-92e4-57babf26cc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (0.4.0).\n",
      "['ask-detrano', 'bak', 'cleve.mod', 'cleveland.data', 'costs', 'heart-disease.names', 'hungarian.data', 'Index', 'long-beach-va.data', 'new.data', 'processed.cleveland.data', 'processed.hungarian.data', 'processed.switzerland.data', 'processed.va.data', 'reprocessed.hungarian.data', 'switzerland.data', 'WARNING']\n"
     ]
    }
   ],
   "source": [
    "path = kagglehub.dataset_download(\"abdelazizsami/heart-disease\")\n",
    "print(os.listdir(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "98be2ffe-3457-4091-85d1-08a0ac34ace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rows = []\n",
    "def parse_heart_data(data_dir, filename):\n",
    "    \"\"\"\n",
    "    Parses the UCI Heart Disease 'stream' data format into a structured DataFrame.\n",
    "    \n",
    "    Identifies records using the 'name' anchor, validates for 76 attributes\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the raw .data file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A validated DataFrame with 76 columns.\n",
    "    \"\"\"\n",
    "    #  Load raw trxt\n",
    "    file_path = f\"{data_dir}/{filename}\"\n",
    "    with open(f'{file_path}') as file:\n",
    "        raw_text = file.read().replace('\\n', ' ')#new line characters are interrupting so i'll strip them or use replace this\n",
    "    raw_rows= raw_text.split('name')\n",
    "\n",
    "    clean_rows = []\n",
    "        \n",
    "    # handle each row  text block(eaah patient record)\n",
    "    for i, record in enumerate(raw_rows):\n",
    "        values = record.strip().split()\n",
    "        \n",
    "        if not values:# this code prevent potential crashes when the anchor 'name' finishes at the end\n",
    "            continue \n",
    "        \n",
    "        # add 'name' back as the 76th column to maintain original data\n",
    "        values.append('name')\n",
    "        \n",
    "        # validation: Check colunm count\n",
    "        if len(values) == 76:\n",
    "            clean_rows.append(values)\n",
    "        else:\n",
    "            error_rows.append({\n",
    "            \"patient_index\": i,\n",
    "            \"found_length\": len(values)\n",
    "            })\n",
    "        \n",
    "    # create DataFrame\n",
    "    df = pd.DataFrame(clean_rows) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6a4b667d-d2cc-4bdf-a2de-f044ea099490",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_files = [\n",
    "    'new.data', #from the kaggle documentation, the cleveland data was corupted and replaced with new.data\n",
    "    'hungarian.data', \n",
    "    'switzerland.data', \n",
    "    'long-beach-va.data'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c9910f0c-d76c-4cb4-882f-68bd2cf99744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data source: new.data with shape: (0, 0)\n",
      "data source: hungarian.data with shape: (294, 76)\n",
      "data source: switzerland.data with shape: (123, 76)\n",
      "data source: long-beach-va.data with shape: (200, 76)\n"
     ]
    }
   ],
   "source": [
    "all_dfs = []\n",
    "for file in my_files:\n",
    "    # We pass 'path' (the KaggleHub directory) into the function\n",
    "    temp_df = parse_heart_data(path, file)\n",
    "\n",
    "    print(f\"data source: {file} with shape: {temp_df.shape}\")\n",
    "\n",
    "    # add source identifier for the master dataset\n",
    "    temp_df['source_dataset'] = file\n",
    "    all_dfs.append(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "430ca3eb-24e6-4069-91d1-f02c1a008067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308\n"
     ]
    }
   ],
   "source": [
    "print(len(error_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6a8b11d4-adb2-4958-a2d7-3f72f10c0d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'patient_index': 0, 'found_length': 90},\n",
       " {'patient_index': 1, 'found_length': 90},\n",
       " {'patient_index': 2, 'found_length': 90},\n",
       " {'patient_index': 3, 'found_length': 90},\n",
       " {'patient_index': 4, 'found_length': 90},\n",
       " {'patient_index': 5, 'found_length': 90}]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_rows[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7994adca-8b7d-4230-8eaa-b0d9da16d92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 errors that aren't length 90:\n",
      "[{'patient_index': 303, 'found_length': 103500}, {'patient_index': 304, 'found_length': 451}, {'patient_index': 305, 'found_length': 721}, {'patient_index': 306, 'found_length': 91}, {'patient_index': 307, 'found_length': 6662}]\n"
     ]
    }
   ],
   "source": [
    "non_new_data_errors = [r for r in error_rows if r.get('found_length') != 90]\n",
    "print(f\"Found {len(non_new_data_errors)} errors that aren't length 90:\")\n",
    "print(non_new_data_errors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c656f87b-9ed5-41f5-9683-76fc329f2451",
   "metadata": {},
   "source": [
    "### the new.data file fails the 76 column chech like we see here then it fails; without any info on how this file's columns are structured then there is no safe way forward, truncating s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "577bf3c7-f154-432e-a99c-20dfb6534a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(all_dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d304d6e7-0af4-4c78-8199-ec2192a22b32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c56a2db2-8dd8-4819-b55c-83a6d3673de0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "# concatenate and Export\n",
    "processed_dfs=[]\n",
    "for i, df in enumerate(all_dfs):\n",
    "    if not df.empty:\n",
    "        df['source_dataset'] = my_files[i].replace('.data', '')\n",
    "        processed_dfs.append(df)\n",
    "master_df = pd.concat(all_dfs, ignore_index=True)# each dataframe will have indexes from 0 to n, and will overlap which defeats the point of being unique thus we set ignore_index to true\n",
    "master_df.to_csv('heart_disease_master.csv', index=False)# index is set to false to avoid point mutation where the indexes are added as data values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca94848-7567-4044-87e8-43ab8eaa85a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "39e77f9c-773f-4c88-9573-dc3bcc8c869d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(617, 77)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e5131e-bf4f-43d1-a596-53b05df60245",
   "metadata": {},
   "source": [
    "### as espected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "90ce0512-62c3-4b62-93d3-0371aaa917dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_dataset</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hungarian</td>\n",
       "      <td>1254</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-9</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-9</td>\n",
       "      <td>-9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-9.</td>\n",
       "      <td>-9.</td>\n",
       "      <td>name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hungarian</td>\n",
       "      <td>1255</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-9</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-9</td>\n",
       "      <td>-9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-9.</td>\n",
       "      <td>-9.</td>\n",
       "      <td>name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hungarian</td>\n",
       "      <td>1256</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-9</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-9</td>\n",
       "      <td>-9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-9.</td>\n",
       "      <td>-9.</td>\n",
       "      <td>name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hungarian</td>\n",
       "      <td>1257</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-9</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>-9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-9.</td>\n",
       "      <td>-9.</td>\n",
       "      <td>name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hungarian</td>\n",
       "      <td>1258</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-9</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-9.</td>\n",
       "      <td>-9.</td>\n",
       "      <td>name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 77 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  source_dataset     0  1   2  3  4  5  6   7  8  ...  66  67 68 69 70 71 72  \\\n",
       "0      hungarian  1254  0  40  1  1  0  0  -9  2  ...  -9  -9  1  1  1  1  1   \n",
       "1      hungarian  1255  0  49  0  1  0  0  -9  3  ...  -9  -9  1  1  1  1  1   \n",
       "2      hungarian  1256  0  37  1  1  0  0  -9  2  ...  -9  -9  1  1  1  1  1   \n",
       "3      hungarian  1257  0  48  0  1  1  1  -9  4  ...   2  -9  1  1  1  1  1   \n",
       "4      hungarian  1258  0  54  1  1  0  1  -9  3  ...   1  -9  1  1  1  1  1   \n",
       "\n",
       "    73   74    75  \n",
       "0  -9.  -9.  name  \n",
       "1  -9.  -9.  name  \n",
       "2  -9.  -9.  name  \n",
       "3  -9.  -9.  name  \n",
       "4  -9.  -9.  name  \n",
       "\n",
       "[5 rows x 77 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5a5504e2-a59d-4df4-ae65-d0aa51990ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'source_dataset'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
